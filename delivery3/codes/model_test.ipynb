{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"musamrc/model_256_256\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "Instruction:\n",
    "{}\n",
    "\n",
    "Input:\n",
    "{}\n",
    "\n",
    "Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import openpyxl\n",
    "\n",
    "# TextStreamer nesnesini oluşturun\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "# Excel dosyasını yükleyin\n",
    "wb = openpyxl.load_workbook('excell_step3.xlsx')\n",
    "sheet = wb.active\n",
    "\n",
    "# Excel dosyasındaki verileri kullanarak modeli çalıştırın\n",
    "for row in range(2, sheet.max_row + 1):\n",
    "    instruction = sheet[f'B{row}'].value\n",
    "    input_text = sheet[f'C{row}'].value\n",
    "\n",
    "    # Model için girdileri hazırlayın\n",
    "    inputs = tokenizer(\n",
    "        [\n",
    "            alpaca_prompt.format(instruction, input_text, \"\")\n",
    "        ],\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Modeli çalıştırın ve sonuçları alın\n",
    "    outputs = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128)\n",
    "\n",
    "    # Sonuçları decode edin ve Excel dosyasına yazın\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # İlk olarak, sadece modelin ürettiği çıktıyı (output) ayıklayın\n",
    "    output_only = result.split(input_text)[-1].strip()\n",
    "    final_output = output_only.replace('Response:', '').strip()\n",
    "\n",
    "    # Son olarak, temizlenmiş çıktıyı Excel dosyasının F sütununa yazdırın\n",
    "    sheet[f'F{row}'].value = final_output\n",
    "\n",
    "\n",
    "\n",
    "# Excel dosyasını kaydedin\n",
    "wb.save('dosyanızın_yolu.xlsx')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
